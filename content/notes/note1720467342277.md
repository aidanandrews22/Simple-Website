# Building a RAG algorithm and vector database for prompt curation

### Steps

## Data preprocessing and embedding creation
- [x] Open the pdf in python
- [ ] format the text so that it is ready for the embedding model
- [ ] embed all chunks of text and turn them into numerical representation (ie. embedding)

## Search and answer algo
- [ ] build retrieval system using vector search to find relevant chunk of data based on prompt
- [ ] create a prompt that incorporates the response from the vector model
- [ ] generate an answer using an open source llm that is passed the updated prompt locally

### Documentation

## 1. Data preprocessing and embedding creation 

- Import PDF and clean
	- ``` python preview title="import pdf"
		lines 10-20
	```
- Convert pdf data into list, then get and store data about the data.
	- ``` python preview title="process data"
		lines 20-46
	```

> [!IMPORTANT]
> # Tokens
> 1: tokens are necessary for the embedding model since it can not work on infinite tokens
> 2: LLMs, similarly to the embedding model, can not work with infinite tokens
>
> So it is essential that we quantize our data in a pretty uniform and universally interpretable
> form
>
> OpenAI has a pretty cool tool that allows you to visualize tokens [here](https://platform.openai.com/tokenizer).

- Process text for embedding (splitting into chunks of sentences)
- Embed text chunks with embedding model
- Save embedding to a file for storage

> Sources
> [RAG Paper](https://arxiv.org/pdf/2005.11401)