# Building a RAG algorithm and vector database for prompt curation

### Steps

## Data preprocessing and embedding creation
- [x] Open the pdf in python
- [x] format the text so that it is ready for the embedding model
- [ ] embed all chunks of text and turn them into numerical representation (ie. embedding)

## Search and answer algo
- [ ] build retrieval system using vector search to find relevant chunk of data based on prompt
- [ ] create a prompt that incorporates the response from the vector model
- [ ] generate an answer using an open source llm that is passed the updated prompt locally

### Documentation

## 1. Data preprocessing

- Import PDF and clean
	- ``` python preview title="import pdf"
		lines 10-20
	```
- Convert pdf data into list, then get and store data about the data.
	- ``` python preview title="process data"
		lines 20-46
	```
- Chunk sentences together
	- Text is easier to filter
	- Chunks fit into embedding model context window (tokens)
	- Allows for inadvertent fine-tuning since the context passed to the model will be more specific/focused
	- Split chunks into items so we can get generation with references

## 2. Embedding chunks of text
- I need to convert the natural language (text) into something the computer can comprehend. In this case the embedding will be from text to numbers
- Embedding is complicated, I am using a [pre-build hugging face model](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1). However, it is good to know that embedding a word to text is not a linear/2D transformation (ie. hello->01) rather a much more complicated process that can be better understood after reading [this](https://vickiboykis.com/what_are_embeddings/index.html).
- Once sample size gets over 10^6 embeddings then implement [vector database](https://en.wikipedia.org/wiki/Vector_database)

## 3. RAG Search and Answer
- What I want here is for the model to retrieve all relevant passages based on the user query.
- Use the retrieved output to augment a prompt to an LLM (open-source) so it can generate an output based on the relevant text.
- Example of this being done. [You can use it here](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) (pre-LLM): ![Example](../assets/embedding_example.png)
- In this implementation we want to primarily utilize semantic search. Specifically, I want the model to retrieve relevant data semantically rather than literally (key word search)

### Create semantic search pipeline
- What I want is for the model to search for a query and get back relevant data from the dataset
- Steps
	- 1. Define a query string
	- 2. Convert from string to embedding
	- 3. Compute a dot product or cosine similarity function between the embeddings and the query embedding. This will find similar vector embeddings and create a score based on how similar the embedding vectors are.
	- 4. Sort results from previous step in descending order
- First Implementation results

> ## Example 1
> - Query: What is a typical undergraduate degree to become software engineer at apple
> - model response: {'page_number': 37, 'sentence_chunk': 'Journey 61: **Software Engineer at Apple** High School: Activities: Coding club, AP Computer Science course. Tools: Codecademy (Python basics), Khan Academy (AP Computer Science prep). **Undergraduate Degree: Degree: Bachelors in Computer Science from University of Iowa.** Projects: Developed a personal finance management app. Internships: Summer internship at a local tech startup. Tools: GitHub (version control), Visual Studio Code (coding IDE), LeetCode (coding practice). First Job: Position: Junior Developer at a mid-sized tech company. Projects: Worked on improving backend systems for e-commerce platform. Tools: Docker (containerization), AWS (cloud services), Jenkins (CI/CD). Advanced Skills Development: Courses: Completed an online Machine Learning course from Coursera.', 'chunk_char_count': 778, 'chunk_word_count': 102, 'chunk_token_count': 194.5, 'embedding': array([ 0.31579384,  0.14388724, -0.2736261 , ..., -0.49289274,
        0.36630484,  0.34612772], dtype=float32)}
> ## Example 2
> - Query: Give me some good highschool activities to become a digital marketer at amazon
> - Output: index 88: {'page_number': 32, 'sentence_chunk': 'Tools: matplotlib (data visualization), seaborn (data visualization), Coursera (online courses). Networking and Job Search: Activities: Attended data science meetups, contributed to data analytics forums. Tools: LinkedIn (networking), GitHub (portfolio), DataCamp (community). Goal: Achieved Position: Data Analyst at Spotify. Sources: Spotify Careers, referral from a meetup, data analytics interview prep on DataCamp. ###Journey 53: **Digital Marketer at Amazon High School: Activities: Yearbook club, AP English Language and Composition.** Tools: Canva (graphic design), Khan Academy (AP English prep). Boot Camp: Program: Digital Marketing Boot Camp at BrainStation. Projects: Developed and executed a digital marketing campaign for a local business. Tools: Google Analytics (web analytics), Hootsuite (social media management), Mailchimp (email marketing).', 'chunk_char_count': 857, 'chunk_word_count': 109, 'chunk_token_count': 214.25, 'embedding': array([ 0.5190658 ,  0.57314616, -0.5351296 , ...,  0.1357911 ,
        0.18607634,  0.6845558 ], dtype=float32)}


> [!IMPORTANT]
> # Tokens
> 1: tokens are necessary for the embedding model since it can not work on infinite tokens
> 2: LLMs, similarly to the embedding model, can not work with infinite tokens
>
> So it is essential that we quantize our data in a pretty uniform and universally interpretable
> form
>
> OpenAI has a pretty cool tool that allows you to visualize tokens [here](https://platform.openai.com/tokenizer).

- Process text for embedding (splitting into chunks of sentences)
- Embed text chunks with embedding model
- Save embedding to a file for storage

### To implement
- LangChain [AI21 Semantic Text Splitter](https://python.langchain.com/v0.1/docs/integrations/document_transformers/ai21_semantic_text_splitter/). Splits sentences by identifying distinct topics that form coherent pieces of text and splits along those.
- Vector database that uses approximate nearest neighbor (this will probably happen when the dataset becomes large enough for it to be necessary ~10^6 embeddings)
	- FAISS is a popular indexing library we may want to use when our dataset gets larger.
		- Indexing libraries essentially search and store data the same way we read and write dictionaries. (ex. if I wanted to search learn in a dictionary I would start by going to the i section then the in section then ind and ...
		- What we are currently doing is dot products on every single embedding in the tensor. Which is honestly not that computationally exhausting at the current scale. On my 4090 I can get about 1.68*10^6 embeddings in exactly 0.00199 seconds. This can be quantified as about  252*10^6 words (estimation)
- Probably the most important thing to implement (based on efficacy) is [mxb reranker](https://www.mixedbread.ai/blog/mxbai-rerank-v1)
	- This will improve the order in which results are ranked. [This specific model](https://huggingface.co/mixedbread-ai/mxbai-rerank-large-v1) has been trained to take search results (ex. top 25 from semantic search) and re-rank them in order from most likely top-1 to least likely.

> Sources
> [RAG Paper](https://arxiv.org/pdf/2005.11401)
