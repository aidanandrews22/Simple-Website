# Building a RAG algorithm and vector database for prompt curation

### Steps

## Data preprocessing and embedding creation
- [x] Open the pdf in python
- [x] format the text so that it is ready for the embedding model
- [ ] embed all chunks of text and turn them into numerical representation (ie. embedding)

## Search and answer algo
- [ ] build retrieval system using vector search to find relevant chunk of data based on prompt
- [ ] create a prompt that incorporates the response from the vector model
- [ ] generate an answer using an open source llm that is passed the updated prompt locally

### Documentation

## 1. Data preprocessing

- Import PDF and clean
	- ``` python preview title="import pdf"
		lines 10-20
	```
- Convert pdf data into list, then get and store data about the data.
	- ``` python preview title="process data"
		lines 20-46
	```
- Chunk sentences together
	- Text is easier to filter
	- Chunks fit into embedding model context window (tokens)
	- Allows for inadvertent fine-tuning since the context passed to the model will be more specific/focused
	- Split chunks into items so we can get generation with references

## 2. Embedding chunks of text
- I need to convert the natural language (text) into something the computer can comprehend. In this case the embedding will be from text to numbers
- Embedding is complicated, I am using a [pre-build hugging face model](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1). However, it is good to know that embedding a word to text is not a linear/2D transformation (ie. hello->01) rather a much more complicated process that can be better understood after reading [this](https://vickiboykis.com/what_are_embeddings/index.html).
- Once sample size gets over 10^6 embeddings then implement [vector database](https://en.wikipedia.org/wiki/Vector_database)

## 3. RAG Search and Answer
- What I want here is for the model to retrieve all relevant passages based on the user query.
- Use the retrieved output to augment a prompt to an LLM (open-source) so it can generate an output based on the relevant text.
- Example of this being done. [You can use it here](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) (pre-LLM): ![Example](../assets/embedding_example.png)
- In this implementation we want to primarily utilize semantic search. Specifically, I want the model to retrieve relevant data semantically rather than literally (key word search)

### Create semantic search pipeline
- What I want is for the model to search for a query and get back relevant data from the dataset
- Steps
	- 1. Define a query string
	- 2. Convert from string to embedding
	- 3. Compute a dot product or cosine similarity function between the embeddings and the query embedding. This will find similar vector embeddings and create a score based on how similar the embedding vectors are.
	- 4. Sort results from previous step in descending order
- First Implementation results

> ## Example 1
> - Query: What is a typical undergraduate degree to become software engineer at apple
> - model response: {'page_number': 37, 'sentence_chunk': 'Journey 61: **Software Engineer at Apple** High School: Activities: Coding club, AP Computer Science course. Tools: Codecademy (Python basics), Khan Academy (AP Computer Science prep). **Undergraduate Degree: Degree: Bachelors in Computer Science from University of Iowa.** Projects: Developed a personal finance management app. Internships: Summer internship at a local tech startup. Tools: GitHub (version control), Visual Studio Code (coding IDE), LeetCode (coding practice). First Job: Position: Junior Developer at a mid-sized tech company. Projects: Worked on improving backend systems for e-commerce platform. Tools: Docker (containerization), AWS (cloud services), Jenkins (CI/CD). Advanced Skills Development: Courses: Completed an online Machine Learning course from Coursera.', 'chunk_char_count': 778, 'chunk_word_count': 102, 'chunk_token_count': 194.5, 'embedding': array([ 0.31579384,  0.14388724, -0.2736261 , ..., -0.49289274,
        0.36630484,  0.34612772], dtype=float32)}
> ## Example 2
> - Query: Give me some good highschool activities to become a digital marketer at amazon
> - Output: index 88: {'page_number': 32, 'sentence_chunk': 'Tools: matplotlib (data visualization), seaborn (data visualization), Coursera (online courses). Networking and Job Search: Activities: Attended data science meetups, contributed to data analytics forums. Tools: LinkedIn (networking), GitHub (portfolio), DataCamp (community). Goal: Achieved Position: Data Analyst at Spotify. Sources: Spotify Careers, referral from a meetup, data analytics interview prep on DataCamp. ###Journey 53: **Digital Marketer at Amazon High School: Activities: Yearbook club, AP English Language and Composition.** Tools: Canva (graphic design), Khan Academy (AP English prep). Boot Camp: Program: Digital Marketing Boot Camp at BrainStation. Projects: Developed and executed a digital marketing campaign for a local business. Tools: Google Analytics (web analytics), Hootsuite (social media management), Mailchimp (email marketing).', 'chunk_char_count': 857, 'chunk_word_count': 109, 'chunk_token_count': 214.25, 'embedding': array([ 0.5190658 ,  0.57314616, -0.5351296 , ...,  0.1357911 ,
        0.18607634,  0.6845558 ], dtype=float32)}

## 4. LLM
- Since I am running an LLM locally size and memory need to be assessed and managed correctly
- I need to build a quantization config which will tell the model what precision to load in (ie. 8bit, 4bit, etc)
- Need a model ID to send to transformers so I can get my model and tokenizer
- Need a tokenizer that turns text into a numerical input that is ready for LLM interpretation (essentially a different form of an embedding model)
- [bits and bytes](https://github.com/bitsandbytes-foundation/bitsandbytes)
- [accelerate](https://huggingface.co/docs/accelerate/index)
- Need [flash attention 2](https://github.com/Dao-AILab/flash-attention) to load the model faster. See diagram:
![flash](https://github.com/Dao-AILab/flash-attention/raw/main/assets/flashattn_banner.jpg)

## 4.1 Loading LLM locally
- I am loading [Gemma 2 9b IT ](https://huggingface.co/google/gemma-2-9b-it) although there are plenty of small models with similar capabilities
- To load I have to instantiate a tokenizer and the model
- Here is the model details:
> **Model Details**
>Gemma2ForCausalLM(
  (model): Gemma2Model(
    (embed_tokens): Embedding(256000, 3584, padding_idx=0)
    (layers): ModuleList(
      (0-41): 42 x Gemma2DecoderLayer(
        (self_attn): Gemma2FlashAttention2(
          (q_proj): Linear(in_features=3584, out_features=4096, bias=False)
          (k_proj): Linear(in_features=3584, out_features=2048, bias=False)
          (v_proj): Linear(in_features=3584, out_features=2048, bias=False)
          (o_proj): Linear(in_features=4096, out_features=3584, bias=False)
          (rotary_emb): Gemma2RotaryEmbedding()
        )
        (mlp): Gemma2MLP(
          (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)
          (up_proj): Linear(in_features=3584, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=3584, bias=False)
          (act_fn): PytorchGELUTanh()
        )
        (input_layernorm): Gemma2RMSNorm()
        (post_attention_layernorm): Gemma2RMSNorm()
        (pre_feedforward_layernorm): Gemma2RMSNorm()
        (post_feedforward_layernorm): Gemma2RMSNorm()
      )
    )
    (norm): Gemma2RMSNorm()
  )
  (lm_head): Linear(in_features=3584, out_features=256000, bias=False)
)
{'model_mem_bytes': 18483433472, 'model_mem_mb': 17627.18, 'model_mem_gb': 17.21}
- so to load this model you need about ~20 gigs of vram (higher since forward pass uses vram)

## 4.2 Generating text with the LLM
- Use the chat template when generating all prompts, since this is an instruct model the template is configured to instruct the model precicely
- ``` python preview title="chat template"
		from transformers import AutoTokenizer, AutoModelForCausalLM
		import transformers
		import torch

		model_id = "google/gemma-2-9b-it"
		dtype = torch.bfloat16

		tokenizer = AutoTokenizer.from_pretrained(model_id)
		model = AutoModelForCausalLM.from_pretrained(
		    model_id,
		    device_map="cuda",
		    torch_dtype=dtype,)

		chat = [
		    { "role": "user", "content": "Write a hello world program" },
		]
		prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)
	``` 

## 5. Prompt Engineering
- Augmenting the prompt based on the output of the RAG search
- There are tons of ways of doing this so implementing and testing is best
- [Prompt Engineering Guide](https://www.promptingguide.ai/), [Brex's Prompt Engineering Guide](https://github.com/brexhq/prompt-engineering), [PROMPT DESIGN AND ENGINEERING: INTRODUCTION AND ADVANCED METHODS](https://arxiv.org/pdf/2401.14423), [Anthropic](https://www.anthropic.com/news/prompt-engineering-for-business-performance)
- Today I am starting by
	- **1.** Giving clear instructions
	- **2.** Giving the model examples of good outputs
	- **3.** Telling the model to use the data directly to answer the question

> [!IMPORTANT]
> # Tokens
> 1: tokens are necessary for the embedding model since it can not work on infinite tokens
> 2: LLMs, similarly to the embedding model, can not work with infinite tokens
>
> So it is essential that we quantize our data in a pretty uniform and universally interpretable
> form
>
> OpenAI has a pretty cool tool that allows you to visualize tokens [here](https://platform.openai.com/tokenizer).

- Process text for embedding (splitting into chunks of sentences)
- Embed text chunks with embedding model
- Save embedding to a file for storage

### To implement
- LangChain [AI21 Semantic Text Splitter](https://python.langchain.com/v0.1/docs/integrations/document_transformers/ai21_semantic_text_splitter/). Splits sentences by identifying distinct topics that form coherent pieces of text and splits along those.
- Vector database that uses approximate nearest neighbor (this will probably happen when the dataset becomes large enough for it to be necessary ~10^6 embeddings)
	- FAISS is a popular indexing library we may want to use when our dataset gets larger.
		- Indexing libraries essentially search and store data the same way we read and write dictionaries. (ex. if I wanted to search learn in a dictionary I would start by going to the i section then the in section then ind and ...
		- What we are currently doing is dot products on every single embedding in the tensor. Which is honestly not that computationally exhausting at the current scale. On my 4090 I can get about 1.68*10^6 embeddings in exactly 0.00199 seconds. This can be quantified as about  252*10^6 words (estimation)
- Probably the most important thing to implement (based on efficacy) is [mxb reranker](https://www.mixedbread.ai/blog/mxbai-rerank-v1)
	- This will improve the order in which results are ranked. [This specific model](https://huggingface.co/mixedbread-ai/mxbai-rerank-large-v1) has been trained to take search results (ex. top 25 from semantic search) and re-rank them in order from most likely top-1 to least likely.

## Problem
- The problem is that my chunks are too small and specialized.
## What I want
- I want each chunk to be an entire journey.
## Example of the problem
- Query: 'Give me some good highschool activities to become a digital marketer at amazon'
- Results:
Score: 246.42398071289062
Rank: 1
Text: "Journey 53: Digital Marketer at Amazon High School: Activities: Yearbook
club, AP English Language and Composition.  Tools: Canva (graphic design), Khan
Academy (AP English prep)."
Page Number: 32
- As you can see fantastic results for this query. However, it splits up the content of the journeys. Which is not what we want because then we have a whole bunch of data that does not contextually pertain to specific journey.
## Potential solutions
- Split every chunk at the work journey (bad: not scalable, or flexible)
	- **Pros**
		- very good on small managed dataset
	- **Cons**
		- not scalable
		- hard coded
		- not flexible. requires data to be identical
- Splitting text by semantic meaning with merge
	- **Pros**
		- very good at splitting data by semantic meaning
		- data will be split very well
	- **Cons**
		- not all journeys are the same length so we run into the problem of having information left out or included in places they shouldn't be
- Use ChatGPT API
	- **Pros**
		- everything would literally be perfect
	- **Cons**
		- expensive
		- token count
		- not efficient
		- not salable
		- even more expensive than you thought the first time
		- computationally exhaustive
- Store all data in a JSON file that stores journeys and their respective data
	- **Pros**
		- would be efficient and accurate
	- **Cons**
		- really difficult to implement from an ML side
	- **Ideas**
		- Run semantic search in waves.
			- To do this though we could assign weights to the levels in the JSON file. Meaning each individual journey would have the highest weight so it takes priority in first wave of semantic search. But then we would need summaries for each one to run semantic search on.
			- Then run semantic search once more on the data under that journey in the JSON file which would hopefully give us what we need to pass to LLM.

> Sources
> [RAG Paper](https://arxiv.org/pdf/2005.11401)
