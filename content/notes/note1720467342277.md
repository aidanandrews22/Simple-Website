# Building a RAG algorithm and vector database for prompt curation

### Steps

## Data preprocessing and embedding creation
- [x] Open the pdf in python
- [x] format the text so that it is ready for the embedding model
- [ ] embed all chunks of text and turn them into numerical representation (ie. embedding)

## Search and answer algo
- [ ] build retrieval system using vector search to find relevant chunk of data based on prompt
- [ ] create a prompt that incorporates the response from the vector model
- [ ] generate an answer using an open source llm that is passed the updated prompt locally

### Documentation

## 1. Data preprocessing

- Import PDF and clean
	- ``` python preview title="import pdf"
		lines 10-20
	```
- Convert pdf data into list, then get and store data about the data.
	- ``` python preview title="process data"
		lines 20-46
	```
- Chunk sentences together
	- Text is easier to filter
	- Chunks fit into embedding model context window (tokens)
	- Allows for inadvertent fine-tuning since the context passed to the model will be more specific/focused
	- Split chunks into items so we can get generation with references

## 2. Embedding chunks of text
- I need to convert the natural language (text) into something the computer can comprehend. In this case the embedding will be from text to numbers
- Embedding is complicated, I am using a [pre-build hugging face model](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1). However, it is good to know that embedding a word to text is not a linear/2D transformation (ie. hello->01) rather a much more complicated process that can be better understood after reading [this](https://vickiboykis.com/what_are_embeddings/index.html).
- Once sample size gets over 10^6 embeddings then implement [vector database](https://en.wikipedia.org/wiki/Vector_database)

## 3. RAG Search and Answer
- What I want here is for the model to retrieve all relevant passages based on the user query.
- Use the retrieved output to augment a prompt to an LLM (open-source) so it can generate an output based on the relevant text.
- Example of this being done. [You can use it here](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) (pre-LLM): ![Example](../assets/embedding_example.png)
- In this implementation we want to primarily utilize semantic search. Specifically, I want the model to retrieve relevant data semantically rather than literally (key word search)

### Create semantic search pipeline
- What I want is for the model to search for a query and get back relevant data from the dataset
- Steps
	- 1. Define a query string
	- 2. Convert from string to embedding
	- 3. Compute a dot product or cosine similarity function between the embeddings and the query embedding. This will find similar vector embeddings and create a score based on how similar the embedding vectors are.
	- 4. Sort results from previous step in descending order
- First Implementation results
> - Query: What is a typical undergraduate degree to become software engineer at apple
> - model response: {'page_number': 37, 'sentence_chunk': 'Journey 61: Software Engineer at Apple High School: Activities: Coding club, AP Computer Science course. Tools: Codecademy (Python basics), Khan Academy (AP Computer Science prep). Undergraduate Degree: Degree: Bachelors in Computer Science from University of Iowa. Projects: Developed a personal finance management app. Internships: Summer internship at a local tech startup. Tools: GitHub (version control), Visual Studio Code (coding IDE), LeetCode (coding practice). First Job: Position: Junior Developer at a mid-sized tech company. Projects: Worked on improving backend systems for e-commerce platform. Tools: Docker (containerization), AWS (cloud services), Jenkins (CI/CD). Advanced Skills Development: Courses: Completed an online Machine Learning course from Coursera.', 'chunk_char_count': 778, 'chunk_word_count': 102, 'chunk_token_count': 194.5, 'embedding': array([ 0.31579384,  0.14388724, -0.2736261 , ..., -0.49289274,
        0.36630484,  0.34612772], dtype=float32)}

> [!IMPORTANT]
> # Tokens
> 1: tokens are necessary for the embedding model since it can not work on infinite tokens
> 2: LLMs, similarly to the embedding model, can not work with infinite tokens
>
> So it is essential that we quantize our data in a pretty uniform and universally interpretable
> form
>
> OpenAI has a pretty cool tool that allows you to visualize tokens [here](https://platform.openai.com/tokenizer).

- Process text for embedding (splitting into chunks of sentences)
- Embed text chunks with embedding model
- Save embedding to a file for storage

### To implement
- LangChain [AI21 Semantic Text Splitter](https://python.langchain.com/v0.1/docs/integrations/document_transformers/ai21_semantic_text_splitter/). Splits sentences by identifying distinct topics that form coherent pieces of text and splits along those.
- Vector database that uses approximate nearest neighbor (this will probably happen when the dataset becomes large enough for it to be necessary ~1*10^6 embeddings)

> Sources
> [RAG Paper](https://arxiv.org/pdf/2005.11401)
