# Building a RAG algorithm and vector database for prompt curation

### Steps

## Data preprocessing and embedding creation
- [x] Open the pdf in python
- [x] format the text so that it is ready for the embedding model
- [ ] embed all chunks of text and turn them into numerical representation (ie. embedding)

## Search and answer algo
- [ ] build retrieval system using vector search to find relevant chunk of data based on prompt
- [ ] create a prompt that incorporates the response from the vector model
- [ ] generate an answer using an open source llm that is passed the updated prompt locally

### Documentation

## 1. Data preprocessing

- Import PDF and clean
	- ``` python preview title="import pdf"
		lines 10-20
	```
- Convert pdf data into list, then get and store data about the data.
	- ``` python preview title="process data"
		lines 20-46
	```
- Chunk sentences together
	- Text is easier to filter
	- Chunks fit into embedding model context window (tokens)
	- Allows for inadvertent fine-tuning since the context passed to the model will be more specific/focused
	- Split chunks into items so we can get generation with references

## 2. Embedding chunks of text
- I need to convert the natural language (text) into something the computer can comprehend. In this case the embedding will be from text to numbers
- Embedding is complicated, I am using a [pre-build hugging face model](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1). However, it is good to know that embedding a word to text is not a linear/2D transformation (ie. hello->01) rather a much more complicated process that can be better understood after reading [this](https://vickiboykis.com/what_are_embeddings/index.html).
- Once sample size gets over 10^6 embeddings then implement [vector database](https://en.wikipedia.org/wiki/Vector_database)

## 3. RAG Search and Answer
- What I want here is for the model to retrieve all relevant passages based on the user query.
- Use the retrieved output to augment a prompt to an LLM (open-source) so it can generate an output based on the relevant text.
- Example of this being done. [You can use it here](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) (pre-LLM): ![Example](../assets/embedding_example.png)
- In this implementation we want to primarily utilize semantic search. Specifically, I want the model to retrieve relevant data semantically rather than literally (key word search)

> [!IMPORTANT]
> # Tokens
> 1: tokens are necessary for the embedding model since it can not work on infinite tokens
> 2: LLMs, similarly to the embedding model, can not work with infinite tokens
>
> So it is essential that we quantize our data in a pretty uniform and universally interpretable
> form
>
> OpenAI has a pretty cool tool that allows you to visualize tokens [here](https://platform.openai.com/tokenizer).

- Process text for embedding (splitting into chunks of sentences)
- Embed text chunks with embedding model
- Save embedding to a file for storage

### To implement
- LangChain [AI21 Semantic Text Splitter](https://python.langchain.com/v0.1/docs/integrations/document_transformers/ai21_semantic_text_splitter/). Splits sentences by identifying distinct topics that form coherent pieces of text and splits along those.
- Vector database that uses approximate nearest neighbor (this will probably happen when the dataset becomes large enough for it to be necessary ~1*10^6 embeddings)

> Sources
> [RAG Paper](https://arxiv.org/pdf/2005.11401)
